# Скрипт для автоматического копирования веб-страниц

Репозиторий создан специально для видео [Как скачать Интернет?](https://youtu.be/dyB4tMp2Bpg) YouTube канала [Псевдо Программист](https://www.youtube.com/channel/UCr6C1YmaQ0FgsfwzIVd8ZEg).
![Превью v2](https://github.com/IgorVolochay/crawler/assets/44619012/ced712e5-f84d-481e-9c76-5eea4468f186)

## Использованные технологии:
- Язык программирования Python 3.9
- beautifulsoup4
- pywebcopy
- requests

## Краулер:
Скрипт краулера работает по алгоритму с использованием рекурсии. Функция `get_all_links` принимает на вход ссылку на сайт, параметр глубины поиска и счётчик текущей глубины поиска (он изначально имеет значение 0).
При помощи библиотек `requests` и `BeautifulSoup` мы получаем HTML код сайта. Программа проходиться по всем гиперссылкам. И для каждой найденной ссылки мы вновь вызываем функцию `get_all_links`, только на этот раз мы к счётчику глубины прибавляем единицу. Когда счётчик превысит установленную глубину поиска, то новые ссылки искаться не будут.

```python
if __name__ == '__main__':
    start_url = " " # Необходимо указать стартовую веб-страницу

    links = get_all_links(start_url, 1) # Необходимо указать глубину поиска
```

## Загрузчик веб-страницы
Для загрузки веб-страниц используется модуль `pywebcopy`, который способен сохранять HTML, CSS и JS выбранной страницы. Работа загрузчика веб-страниц представлена в функции `save_page`, которая принимает на вход ссылку. Помимо этого, необходимо указать путь до папки, куда будут сохраняться загруженные веб-страницы.

```python
    save_webpage(
        url=url,
        project_folder="", # Путь до папки, куда будут сохраняться загруженные веб-страницы
        project_name=str(dir_name),
        open_in_browser=False, 
        threaded=True # Реализация асинхронной работы загрузчика
    )
```
